{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Fragment Error Analysis 1\n",
    "Date: January 11, 2021\n",
    "## Objectives\n",
    "Isolate fragment impact from fragment frequency.  The idea is to minimize the impact of highly frequent fragments\n",
    "such as `ccc`.\n",
    "\n",
    "### Approach\n",
    "1. Split molecules into \"easy to predict\" and \"hard to predict\"\n",
    "    1. Top and bottom quartiles of scaled average error\n",
    "    2. This might need to be **dataset specific**.  Molecules or fragments that are difficult to predict for one\n",
    "      property may not be difficult for the next.  These effects will offset in an average error.\n",
    "      Try logP14k without scaled error in next attempt.\n",
    "\n",
    "2. Compare and contrast fragments from these groups.\n",
    "    1. Are the most common (by number of appearances) the same?\n",
    "\n",
    "3. Remove highly conserved fragments.  Fragments that are present in both in easy and hard to predict molecule sets\n",
    " are removed.\n",
    "    1. This might remove all fragments?\n",
    "    2. Maybe remove the top `n` most frequent or the top `X%` most frequent\n",
    "\n",
    "4. Identify which fragments are most popular based on relationship counts and relationship error weights.\n",
    "\n",
    "\n",
    "5.  Analyze results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping Molecules by Error Prediction Error\n",
    "I need to collect statistics on molecules average prediction errors.  For simplicity and minimizing variables,\n",
    "I am going to just use the `Lipophilicity` dataset.  \n",
    "\n",
    "**Make a difficulty property based on molecule predictions.**  This will be used to categorize molecules as hard to predict.  In the below query, I do not used the `scaled average error` because I am only looking at a single dataset.  This is not ideal since I am writing directly to the molecule node, which may be a part of more than one dataset.\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[]-(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)\n",
    "WITH avg(p.average_error) as difficulty, M, T, p\n",
    "SET M.difficulty = difficulty\n",
    "RETURN M,T, p\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the difficult to predict molecules.**  This query will find the molecules above the 90th percentile.  In other words, the 10% of molecules with the highest average error.\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WHERE M.difficulty > cutoff\n",
    "RETURN  id(M) as NodeID, M.smiles as SMILES , M.difficulty as Difficulty, cutoff ORDER BY\n",
    "M.difficulty DESC LIMIT 100\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find the most common fragments.** We want to subtract the most common fragments in the bottom 90% from the fragments in the top 10% most difficult molecules.  But first we must identify what fragments are most common.\n",
    "\n",
    "**Most common fragments in easy molecules**\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff, count(M) as total\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WHERE M.difficulty < cutoff\n",
    "RETURN F.name, count(f), 0.9 * total as Total, toFloat(count(f)) / 0.9 / total * 100 as percent ORDER BY count(f) DESC LIMIT 100\n",
    "```\n",
    "**Most common fragments in hard molecules**\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff, count(M) as total\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WHERE M.difficulty > cutoff\n",
    "RETURN F.name, count(f), 0.1 * total as Total, toFloat(count(f)) / 0.1 / total * 100 as percent ORDER BY count(f) DESC LIMIT 100\n",
    "```\n",
    "\n",
    "The above Cypher command finds the most frequent fragments in the group based on number of relationships it has to molecules in the dataset.  It calculates the percent of molecules in that group that have that fragment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing fragments that overlap molecular groups\n",
    "Next we need to find what fragments are common in both the high error and less error sets.  Then isolate the ones more frequent in the high error group.\n",
    "\n",
    "I think there are several ways we could go about making rules for which fragments to remove.\n",
    "\n",
    "1. We could remove the `n` most common fragments in the easy group from the hard group.\n",
    "\n",
    "2. Remove fragments with a prevelence above a threshold, say 25%.  i.e if a fragment is present in 25% or more of the easy molecules, remove it.\n",
    "\n",
    "3. We could remove fragments that have the same prevelence (within a threshold, say 2%) in both the hard and easy sets.\n",
    "\n",
    "4. Remove all fragments present in the easy group from the hard group.  This will remove the most and leave fragments that *only* exist in the hard group.\n",
    "\n",
    "Let's start with the first approach."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "##  Fragment Analysis\n",
    "Ideally, we would be able to calculate molecule `difficulty` on the fly when running the analysis.   A user may want to know what fragments are difficult for a particular chemical property, such as logP.  In this scenario, the `difficulty` property should only consider logP errors.  But then we have a user-query specific property persisting in the mother graph, which is undesired.  My less than elegant solution is as follows:\n",
    "1. Remove all `difficulty` weights\n",
    "2. Make new `difficulty` weights for the chemical property of interest\n",
    "3. Run Fragment Analysis\n",
    "4. Remove the `difficulty` weights"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlined Cypher Procedure\n",
    "Cypher commands can be run in Batch using `;` to separate the commands, but the outputs will be suppressed.  So the command that returns your results should be run by itself.  These first 3 commands can be run together, however. They remove old weights, set the dataset of interest, and create new weights for use with the analysis.  \n",
    "\n",
    "### Prepare Graph\n",
    "```cypher\n",
    "// Delete old weights\n",
    "MATCH (D:DataSet)-[:SPLITS_INTO_TEST]->(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WITH avg(p.average_error) as difficulty, f, M, F\n",
    "REMOVE M.difficulty, f.difficulty                      \n",
    "RETURN M, F, f LIMIT 20;\n",
    "\n",
    "// Set the Dataset you are interested in\n",
    ":param data => \"ESOL.csv\"; // must be in separate command from MATCH\n",
    "\n",
    "// Make new weights for LogP\n",
    "MATCH (D:DataSet{data: $data})-[:SPLITS_INTO_TEST]->(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WITH avg(p.average_error) as difficulty, f, M, F\n",
    "SET M.difficulty = difficulty                      \n",
    "SET f.difficulty = difficulty\n",
    "RETURN M, F, f LIMIT 20;\n",
    "```\n",
    "### Run Fragment Analysis\n",
    "The command below produces the fragment analysis and returns the number of relationships, the sum of their errors and the average error.\n",
    "\n",
    "```cypher\n",
    "// Remove Common Fragments\n",
    "MATCH (D:DataSet{data: $data})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff\n",
    "\n",
    "MATCH (D:DataSet{data: $data})-[c:CONTAINS_MOLECULE]->(eM:Molecule)-[ef:HAS_FRAGMENT]->(eF:Fragment)\n",
    "WHERE eM.difficulty < cutoff // easy molecules\n",
    "WITH eF, count(ef) as efreq, cutoff // gath frags and frequency\n",
    "ORDER BY efreq DESC LIMIT 1000  //  limit to top n\n",
    "WITH  collect(eF) as easyFrags, cutoff\n",
    "\n",
    "MATCH (D:DataSet{data: $data})-[c:CONTAINS_MOLECULE]->(hM:Molecule)-[hf:HAS_FRAGMENT]->(hF:Fragment)\n",
    "WHERE hM.difficulty > cutoff // hard molecules\n",
    "WITH hF, count(hf) as hfreq, easyFrags\n",
    "ORDER BY hfreq DESC LIMIT 1000\n",
    "WITH collect(hF) as hardFrags, easyFrags\n",
    "\n",
    "// use APOC to do list intersect & subtraction\n",
    "WITH apoc.coll.intersection(easyFrags, hardFrags) as overlap, apoc.coll.subtract(hardFrags, easyFrags) as remain \n",
    "UNWIND remain as rFrags\n",
    "MATCH (M:Molecule)-[f:HAS_FRAGMENT]->(rFrags)\n",
    "                                                               \n",
    "// Get Difficulty Stats for Remaining Fragments\n",
    "WITH rFrags.name as fragment, count(f) as number_of_rel, sum(f.difficulty) as sum_difficulty,sum(f.difficulty)/count(f) as avg_difficulty\n",
    "RETURN fragment, number_of_rel, sum_difficulty, avg_difficulty\n",
    "ORDER BY number_of_rel DESC, avg_difficulty DESC   \n",
    "                                                               \n",
    "```\n",
    "### Clean up\n",
    "***RUN THIS AT THE END to clean up after yourself!***\n",
    "\n",
    "```cypher\n",
    "// Delete weights again\n",
    "MATCH (D:DataSet{data: $data})-[:SPLITS_INTO_TEST]->(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WITH avg(p.average_error) as difficulty, f, M, F\n",
    "REMOVE M.difficulty = difficulty                      \n",
    "REMOVE f.difficulty = difficulty\n",
    "RETURN M, F, f LIMIT 20;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = '/home/adam/research/neo4j/gds_results/pageRank/'\n",
    "df_w = pd.read_csv(path + 'mol_frags_weight_3.csv')\n",
    "df_n = pd.read_csv(path + 'mol_frags_noweight_3.csv')\n",
    "\n",
    "n = 5000  # counter for n highest results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_n.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up Results\n",
    "We need to merge the two results into one dataframe.\n",
    "One column with the fragment, one with unweighted scores,\n",
    "one with the weighted scores.\n",
    "\n",
    "Once that is done, we can see how we can manipulate them to get answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.merge(df_n, df_w, on=\"name\", how='outer', suffixes=(\"_no_weight\", \"_weight\"))\n",
    "df.dropna()\n",
    "df[\"score_diff\"] = df[\"score_weight\"] - df[\"score_no_weight\"]\n",
    "\n",
    "# Negative rank diff means fragment became more important with weight!\n",
    "df[\"rank_diff\"] = df[\"rank_weight\"] - df[\"rank_no_weight\"]\n",
    "\n",
    "# Positive frac means fragment became more important with weight\n",
    "df[\"frac\"] = (df[\"score_weight\"] - df[\"score_no_weight\"])/df[\"score_no_weight\"]*100\n",
    "# df = df[np.abs(df.score_diff) > 0.01]\n",
    "df.sort_values(by=\"score_diff\", ascending=True).head(25)\n",
    "# df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.style.use('bmh')\n",
    "dff=df[:1000]\n",
    "fig, ax = plt.subplots()\n",
    "# ax = plt.subplot(111)\n",
    "ax.bar(dff.rank_no_weight, dff.rank_diff, color=(dff['rank_diff'] > 0).map({True: 'b', False: 'r'}))\n",
    "plt.xlabel(\"Unweighted Fragment Rank\")\n",
    "plt.ylabel(\"Rank Change After Weighting\")\n",
    "# plt.ylim(-225, 225)\n",
    "plt.xlim(0,)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the number of points the graph looks at.  Interestingly, you can see that as `n` increases, the `rank_diff` tends to also increase.  This is largely because very little score separates entries at high ranks (low score).  So a small delta in score can cause a large jump in rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dff=df[:2500]  # adjust what data to look at here\n",
    "plt.style.use('bmh')\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(dff.score_no_weight, dff.score_weight)\n",
    "plt.xlabel(\"Unweighted Fragment Score\")\n",
    "plt.ylabel(\"Weighted Fragment Score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whole Graph Page Rank\n",
    "I figured it would be interesting to run the PageRank algorithm on the full graph, unweighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the graph projection\n",
    "```cypher\n",
    "CALL gds.graph.create('whole-graph', '*', '*')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run PageRank Algorithm\n",
    "```cypher\n",
    "CALL gds.pageRank.stream('whole-graph',{\n",
    "    maxIterations: 20\n",
    "    })\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).name AS name, score,  labels(gds.util.asNode(nodeId)) as NodeType, nodeId\n",
    "ORDER BY NodeType, score DESC\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}