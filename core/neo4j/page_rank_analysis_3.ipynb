{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Page Rank Analysis 2\n",
    "Date: October 12, 2020\n",
    "## Objectives\n",
    "Isolate fragment impact from fragment frequency.  The idea is to minimize the impact of highly frequent fragments\n",
    "such as `ccc`.\n",
    "\n",
    "### Approach\n",
    "1. Split molecules into \"easy to predict\" and \"hard to predict\"\n",
    "    1. Top and bottom quartiles of scaled average error\n",
    "    2. This might need to be **dataset specific**.  Molecules or fragments that are difficult to predict for one\n",
    "      property may not be difficult for the next.  These effects will offset in an average error.\n",
    "      Try logP14k without scaled error in next attempt.\n",
    "\n",
    "2. Compare and contrast fragments from these groups.\n",
    "    1. Are the most common (by number of appearances) the same?\n",
    "\n",
    "3. Remove highly conserved fragments.  Fragments that are present in both in easy and hard to predict molecule sets\n",
    " are removed.\n",
    "    1. This might remove all fragments?\n",
    "    2. Maybe remove the top `n` most frequent or the top `X%` most frequent\n",
    "\n",
    "4. Create graph projection with remaining molecules and fragments. Create unweighted and weighted graphs.\n",
    "\n",
    "5. Run PageRank algorithm on both graph projections.\n",
    "    1. Return fragments rank and score.  Collect results in CSV.\n",
    "\n",
    "6.  Analyze results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Grouping Molecules by Error Prediction Error\n",
    "I need to collect statistics on molecules average prediction errors.  For simplicity and minimizing variables,\n",
    "I am going to just use the `Lipophilicity` dataset.  Currently, there are 11 models that have used this dataset.\n",
    "\n",
    "**Make a difficulty property based on molecule predictions.**  This will be used to categorize molecules as hard to predict.  In the below query, I do not used the `scaled average error` because I am only looking at a single dataset.  This is not ideal since I am writing directly to the molecule node, which may be a part of more than one dataset.\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[]-(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)\n",
    "WITH avg(p.average_error) as difficulty, M, T, p\n",
    "SET M.difficulty = difficulty\n",
    "RETURN M,T, p\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Find the difficult to predict molecules.**  This query will find the molecules above the 90th percentile.  In other words, the 10% of molecules with the highest average error.\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WHERE M.difficulty > cutoff\n",
    "RETURN  id(M) as NodeID, M.smiles as SMILES , M.difficulty as Difficulty, cutoff ORDER BY\n",
    "M.difficulty DESC LIMIT 100\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Find the most common fragments.** We want to subtract the most common fragments in the bottom 90% from the fragments in the top 10% most difficult molecules.  But first we must identify what fragments are most common.\n",
    "\n",
    "**Most common fragments in easy molecules**\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff, count(M) as total\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WHERE M.difficulty < cutoff\n",
    "RETURN F.name, count(f), 0.9 * total as Total, toFloat(count(f)) / 0.9 / total * 100 as percent ORDER BY count(f) DESC LIMIT 100\n",
    "```\n",
    "**Most common fragments in hard molecules**\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff, count(M) as total\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WHERE M.difficulty > cutoff\n",
    "RETURN F.name, count(f), 0.1 * total as Total, toFloat(count(f)) / 0.1 / total * 100 as percent ORDER BY count(f) DESC LIMIT 100\n",
    "```\n",
    "\n",
    "The above Cypher command finds the most frequent fragments in the group based on number of relationships it has to molecules in the dataset.  It calculates the percent of molecules in that group that have that fragment."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Removing fragments that overlap molecular groups\n",
    "Next we need to find what fragments are common in both the high error and less error sets.  Then isolate the ones more frequent in the high error group.\n",
    "\n",
    "I think there are several ways we could go about making rules for which fragments to remove.\n",
    "\n",
    "1. We could remove the `n` most common fragments in the easy group from the hard group.\n",
    "\n",
    "2. Remove fragments with a prevelence above a threshold, say 25%.  i.e if a fragment is present in 25% or more of the easy molecules, remove it.\n",
    "\n",
    "3. We could remove fragments that have the same prevelence (within a threshold, say 2%) in both the hard and easy sets.\n",
    "\n",
    "4. Remove all fragments present in the easy group from the hard group.  This will remove the most and leave fragments that *only* exist in the hard group.\n",
    "\n",
    "Let's start with the first approach."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```cypher\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(M:Molecule)\n",
    "WITH  percentileCont(M.difficulty, 0.90) as cutoff\n",
    "\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(eM:Molecule)-[ef:HAS_FRAGMENT]->(eF:Fragment)\n",
    "WHERE eM.difficulty < cutoff // easy molecules\n",
    "WITH eF, count(ef) as efreq, cutoff // gath frags and frequency\n",
    "ORDER BY efreq DESC LIMIT 1000  //  limit to top n\n",
    "WITH  collect(eF) as easyFrags, cutoff\n",
    "\n",
    "MATCH (D:DataSet{data:'Lipophilicity-ID.csv'})-[c:CONTAINS_MOLECULE]->(hM:Molecule)-[hf:HAS_FRAGMENT]->(hF:Fragment)\n",
    "WHERE hM.difficulty > cutoff // hard molecules\n",
    "WITH hF, count(hf) as hfreq, easyFrags\n",
    "ORDER BY hfreq DESC LIMIT 1000\n",
    "WITH collect(hF) as hardFrags, easyFrags\n",
    "\n",
    "WITH apoc.coll.intersection(easyFrags, hardFrags) as overlap, apoc.coll.subtract(hardFrags, easyFrags) as remain  // use APOC to do list intersect & subtraction\n",
    "RETURN size(remain), remain\n",
    "```\n",
    "\n",
    "The above query collects the 1000 most frequent fragments in both the easy and difficult groups.  Then it calculates the overlap between the sets and the difference between them. It returns what remains of the hard group once the easy fragments have been removed.\n",
    "\n",
    "You can `UNWIND` the resulting list to use the nodes in a `MATCH` query.\n",
    "```cypher\n",
    "UNWIND remain as rFrags\n",
    "MATCH (M:Molecule)-[:HAS_FRAGMENT]->(rFrags)\n",
    "RETURN M, rFrags\n",
    "```\n",
    "\n",
    "This will return the molecules that have those \"difficult\" fragments."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##  Graph Projections\n",
    "We are going to use a Cypher graph projection. Ideally, we would be able to calculate molecule `difficulty` on the fly when creating the graph projection.  However, the Cypher commands for graph projection must be read-only.  So the `difficulty` property must exsist prior to projection.  The challenge with this is that the graph analysis may be focused at a certain dataset.  A user may want to know what fragments are difficult for a particular chemical property, such as logP.  In this scenario, the `difficulty` property should only consider logP errors.  But then we have a user-query specific property persisting in the mother graph, which is undesired.  My less than elegant solution is as follows:\n",
    "1. Remove all `difficulty` weights\n",
    "2. Make new `difficulty` weights for the chemical property of interest\n",
    "3. Create the Cypher graph projection(s)\n",
    "4. Remove the `difficulty` weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Remove Old Weights\n",
    "```cypher\n",
    "MATCH (:Molecule)-[h:HAS_FRAGMENT]->(:Fragment)\n",
    "REMOVE h.difficulty\n",
    "RETURN h limit 10\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make New Weights for LogP\n",
    "```cypher\n",
    "MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"})-[:SPLITS_INTO_TEST]->(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WITH avg(p.average_error) as difficulty, f, M, F\n",
    "SET f.difficulty = difficulty\n",
    "RETURN M, F, f LIMIT 20\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Create Graph Projection\n",
    "This query contains the same logic as the above sections: set a difficulty cutoff (90 percentile), remove common fragments, etc.  This just does it all in one, big, ugly command.\n",
    "```cypher\n",
    "CALL gds.graph.create.cypher(\n",
    "    'hard-frags-weight',\n",
    "    'MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"}) MATCH (D)-[:CONTAINS_MOLECULE]->(mol:Molecule)<-[stats:CONTAINS_PREDICTED_MOLECULE]-(:TestSet) WITH avg(stats.scaled_average_error) as aces, mol.smiles as smiles WITH percentileCont(aces, 0.90) as cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(eM:Molecule) WITH avg(stats.scaled_average_error) as ace, eM.smiles as smiles, cutoff UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(easy_frags:Fragment) WHERE row[\"ace\"] < cutoff WITH collect(easy_frags) as easy_frags, cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(hM:Molecule) WITH avg(stats.scaled_average_error) as ace, hM.smiles as smiles, cutoff, easy_frags UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(hard_frags:Fragment) WHERE row[\"ace\"] > cutoff WITH collect(hard_frags) as hard_frags, easy_frags WITH apoc.coll.subtract(hard_frags, easy_frags) as hard_frags MATCH (mol:Molecule) WITH collect(mol) as molecules, hard_frags UNWIND [hard_frags, molecules] as sublist UNWIND sublist as mol_or_hard_frag RETURN id(mol_or_hard_frag) as id',\n",
    "    'MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"}) MATCH (D)-[:CONTAINS_MOLECULE]->(mol:Molecule)<-[stats:CONTAINS_PREDICTED_MOLECULE]-(:TestSet) WITH avg(stats.scaled_average_error) as aces, mol.smiles as smiles WITH percentileCont(aces, 0.90) as cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(eM:Molecule) WITH avg(stats.scaled_average_error) as ace, eM.smiles as smiles, cutoff UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(easy_frags:Fragment) WHERE row[\"ace\"] < cutoff WITH collect(easy_frags) as easy_frags, cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(hM:Molecule) WITH avg(stats.scaled_average_error) as ace, hM.smiles as smiles, cutoff, easy_frags UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(hard_frags:Fragment) WHERE row[\"ace\"] > cutoff WITH collect(hard_frags) as hard_frags, easy_frags WITH apoc.coll.subtract(hard_frags, easy_frags) as hard_frags UNWIND hard_frags as hard_frag MATCH (T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(mol:Molecule)-[f:HAS_FRAGMENT]->(hard_frag) RETURN id(mol) AS source, id(hard_frag) AS target, f.difficulty as weight'\n",
    ")\n",
    "YIELD graphName, nodeCount, relationshipCount;\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Single Command for Procedure\n",
    "All of the Cypher steps can be run at once by separating them with `;`\n",
    "\n",
    "```cypher\n",
    "// Delete old weights\n",
    "MATCH (:Molecule)-[h:HAS_FRAGMENT]->(:Fragment)\n",
    "REMOVE h.difficulty\n",
    "RETURN h limit 10;\n",
    "\n",
    "// Make new weights for LogP\n",
    "MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"})-[:SPLITS_INTO_TEST]->(T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(M:Molecule)-[f:HAS_FRAGMENT]->(F:Fragment)\n",
    "WITH avg(p.average_error) as difficulty, f, M, F\n",
    "SET f.difficulty = difficulty\n",
    "RETURN count(f);\n",
    "\n",
    "// Create the graph projection\n",
    "CALL gds.graph.create.cypher(\n",
    "    'hard-frags-weight',\n",
    "    'MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"}) MATCH (D)-[:CONTAINS_MOLECULE]->(mol:Molecule)<-[stats:CONTAINS_PREDICTED_MOLECULE]-(:TestSet) WITH avg(stats.scaled_average_error) as aces, mol.smiles as smiles WITH percentileCont(aces, 0.90) as cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(eM:Molecule) WITH avg(stats.scaled_average_error) as ace, eM.smiles as smiles, cutoff UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(easy_frags:Fragment) WHERE row[\"ace\"] < cutoff WITH collect(easy_frags) as easy_frags, cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(hM:Molecule) WITH avg(stats.scaled_average_error) as ace, hM.smiles as smiles, cutoff, easy_frags UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(hard_frags:Fragment) WHERE row[\"ace\"] > cutoff WITH collect(hard_frags) as hard_frags, easy_frags WITH apoc.coll.subtract(hard_frags, easy_frags) as hard_frags MATCH (mol:Molecule) WITH collect(mol) as molecules, hard_frags UNWIND [hard_frags, molecules] as sublist UNWIND sublist as mol_or_hard_frag RETURN id(mol_or_hard_frag) as id',\n",
    "    'MATCH (D:DataSet{data:\"Lipophilicity-ID.csv\"}) MATCH (D)-[:CONTAINS_MOLECULE]->(mol:Molecule)<-[stats:CONTAINS_PREDICTED_MOLECULE]-(:TestSet) WITH avg(stats.scaled_average_error) as aces, mol.smiles as smiles WITH percentileCont(aces, 0.90) as cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(eM:Molecule) WITH avg(stats.scaled_average_error) as ace, eM.smiles as smiles, cutoff UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(easy_frags:Fragment) WHERE row[\"ace\"] < cutoff WITH collect(easy_frags) as easy_frags, cutoff MATCH (:TestSet)-[stats:CONTAINS_PREDICTED_MOLECULE]->(hM:Molecule) WITH avg(stats.scaled_average_error) as ace, hM.smiles as smiles, cutoff, easy_frags UNWIND [{ace: ace, smiles: smiles}] as row MATCH (:Molecule {smiles: row[\"smiles\"]})-[:HAS_FRAGMENT]->(hard_frags:Fragment) WHERE row[\"ace\"] > cutoff WITH collect(hard_frags) as hard_frags, easy_frags WITH apoc.coll.subtract(hard_frags, easy_frags) as hard_frags UNWIND hard_frags as hard_frag MATCH (T:TestSet)-[p:CONTAINS_PREDICTED_MOLECULE]->(mol:Molecule)-[f:HAS_FRAGMENT]->(hard_frag) RETURN id(mol) AS source, id(hard_frag) AS target, f.difficulty as weight'\n",
    ")\n",
    "YIELD graphName, nodeCount, relationshipCount;\n",
    "\n",
    "// Delete weights again\n",
    "MATCH (:Molecule)-[h:HAS_FRAGMENT]->(:Fragment)\n",
    "REMOVE h.difficulty\n",
    "RETURN h limit 10\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Unweighted PageRank Algorithm\n",
    "\n",
    "```cypher\n",
    "CALL gds.pageRank.stream('mols_native',{\n",
    "\tmaxIterations: 20\n",
    "    })\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).name AS name, score\n",
    "ORDER BY score DESC\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Weighted PageRank Algorithm\n",
    "```cypher\n",
    "CALL gds.pageRank.stream('mols_native',{\n",
    "\tmaxIterations: 20,\n",
    "    relationshipWeightProperty: 'weight'\n",
    "    })\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).name AS name, score\n",
    "ORDER BY score DESC\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "path = '/home/adam/research/neo4j/gds_results/pageRank/'\n",
    "df_w = pd.read_csv(path + 'mol_frags_weight_3.csv')\n",
    "df_n = pd.read_csv(path + 'mol_frags_noweight_3.csv')\n",
    "\n",
    "n = 5000  # counter for n highest results"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_n.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_w.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cleanup(df, n = 0):\n",
    "    \"\"\"\n",
    "    Quick function to process incoming dataframes.\n",
    "    Accepts a data frame and a integer for how many entries to keep.\n",
    "    \"\"\"\n",
    "    df = df.dropna()\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"rank\"] = df.index + 1\n",
    "    if n == 0:\n",
    "        n = df.shape[0]\n",
    "    df = df[:n]\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# df_w[\"rank\"] = df_w.index + 1\n",
    "# df_w = df_w[:n]\n",
    "# df_w.head(10)\n",
    "df_w = cleanup(df_w)\n",
    "df_w.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_n = cleanup(df_n)\n",
    "df_n.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_n.head(15)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean up Results\n",
    "We need to merge the two results into one dataframe.\n",
    "One column with the fragment, one with unweighted scores,\n",
    "one with the weighted scores.\n",
    "\n",
    "Once that is done, we can see how we can manipulate them to get answers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.merge(df_n, df_w, on=\"name\", how='outer', suffixes=(\"_no_weight\", \"_weight\"))\n",
    "df.dropna()\n",
    "df[\"score_diff\"] = df[\"score_weight\"] - df[\"score_no_weight\"]\n",
    "\n",
    "# Negative rank diff means fragment became more important with weight!\n",
    "df[\"rank_diff\"] = df[\"rank_weight\"] - df[\"rank_no_weight\"]\n",
    "\n",
    "# Positive frac means fragment became more important with weight\n",
    "df[\"frac\"] = (df[\"score_weight\"] - df[\"score_no_weight\"])/df[\"score_no_weight\"]*100\n",
    "# df = df[np.abs(df.score_diff) > 0.01]\n",
    "df.sort_values(by=\"score_diff\", ascending=True).head(25)\n",
    "# df.head(25)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.style.use('bmh')\n",
    "dff=df[:1000]\n",
    "fig, ax = plt.subplots()\n",
    "# ax = plt.subplot(111)\n",
    "ax.bar(dff.rank_no_weight, dff.rank_diff, color=(dff['rank_diff'] > 0).map({True: 'b', False: 'r'}))\n",
    "plt.xlabel(\"Unweighted Fragment Rank\")\n",
    "plt.ylabel(\"Rank Change After Weighting\")\n",
    "# plt.ylim(-225, 225)\n",
    "plt.xlim(0,)\n",
    "# plt.show()\n",
    "# plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Change the number of points the graph looks at.  Interestingly, you can see that as `n` increases, the `rank_diff` tends to also increase.  This is largely because very little score separates entries at high ranks (low score).  So a small delta in score can cause a large jump in rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dff=df[:2500]  # adjust what data to look at here\n",
    "plt.style.use('bmh')\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(dff.score_no_weight, dff.score_weight)\n",
    "plt.xlabel(\"Unweighted Fragment Score\")\n",
    "plt.ylabel(\"Weighted Fragment Score\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Whole Graph Page Rank\n",
    "I figured it would be interesting to run the PageRank algorithm on the full graph, unweighted."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating the graph projection\n",
    "```cypher\n",
    "CALL gds.graph.create('whole-graph', '*', '*')\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run PageRank Algorithm\n",
    "```cypher\n",
    "CALL gds.pageRank.stream('whole-graph',{\n",
    "    maxIterations: 20\n",
    "    })\n",
    "YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).name AS name, score,  labels(gds.util.asNode(nodeId)) as NodeType, nodeId\n",
    "ORDER BY NodeType, score DESC\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}